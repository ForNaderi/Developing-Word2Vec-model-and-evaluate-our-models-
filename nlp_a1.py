# -*- coding: utf-8 -*-
"""NLP_A1

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1x0xQen4u_7vfNSBhbZb129_YQIcqrtHu
"""

import numpy as np
import matplotlib.pyplot as plt
import gensim.downloader as api
import gensim
import nltk
import A1_helper
import os
from gensim import models
import re
#Preprocessing
nltk.download('punkt')
#Download WordNet
nltk.download('wordnet')

nltk.download('conll2000')

sentences = nltk.corpus.conll2000.sents()
# Remove numbers using regular expressions for each sentence
sentences = [[re.sub(r'\d+', '', word) for word in sentence] for sentence in sentences]
nltk.download('punkt')
nltk.download('wordnet')

# Tokenize sentences
a = [nltk.sent_tokenize(' '.join(sent)) for sent in sentences]
#Tokenize words
b = [nltk.word_tokenize(''.join(s)) for s in a]

b = [[w.lower() for w in s]  for s in b]


#Lemmatize
lemmatizer = nltk.WordNetLemmatizer()
b = [[lemmatizer.lemmatize(w) for w in s]  for s in b]

model_path ="/content/model1.txt"
model_path2 = "/content/model2.txt"

if not os.path.exists(model_path):
      m1 = models.Word2Vec(sentences, sg=1, epochs=300, vector_size= 100, min_count=5) #skipgram method
      m1.wv.save_word2vec_format('/content/model1.txt')

      m2 = models.Word2Vec(sentences, sg=0, epochs=300, vector_size= 100, min_count=5) #CBOW method
      m2.wv.save_word2vec_format('/content/model2.txt')
      # Load the model from the .txt file
      m1 = models.KeyedVectors.load_word2vec_format(model_path, binary=False)
      m2 = models.KeyedVectors.load_word2vec_format(model_path2, binary=False)
else:

      # Load the model from the .txt file
      m1 = models.KeyedVectors.load_word2vec_format(model_path, binary=False)
      m2 = models.KeyedVectors.load_word2vec_format(model_path2, binary=False)

#print("vector of word : ", WV["king"])
'''
print("similarity between August and July :", m2.similarity("August", "July"))
print("similarity between August and tomorrow :", m2.similarity("August", "tomorrow"))

print("10 most similar words to Dollar: " , m2.most_similar("dollar", topn=10))

V = (m2["August"]-m2["July"]+m2["tomorrow"])
#print("subtraction of words: ", V)

print("most similar words to V: ", m2.similar_by_vector(V, topn=5))
'''

#print("vector of word : ", WV["king"])
'''
print("similarity between August and July :", m1.similarity("August", "July"))
print("similarity between August and tomorrow :", m1.similarity("August", "tomorrow"))

print("10 most similar words to Dollar: " , m1.most_similar("dollar", topn=10))

V = (m1["August"]-m1["July"]+m1["tomorrow"])
#print("subtraction of words: ", V)

print("most similar words to V: ", m1.similar_by_vector(V, topn=5))
'''

X_vals, Y_vals, labels = A1_helper.reduce_dimensions(m2)
A1_helper.plot_with_matplotlib(X_vals, Y_vals, labels, ["market", "share", "improvement", "increase", "economy", "country", "decrease", "city", "food", "desk",
                                                         "inflation", "human", "finance", "school", "demand", "cash", "bookshelves", "capitaslism", "charge", "power"])

X_vals, Y_vals, labels = A1_helper.reduce_dimensions(m1)
A1_helper.plot_with_matplotlib(X_vals, Y_vals, labels, ["market", "share", "improvement", "increase", "economy", "country", "decrease", "city", "food", "desk",
                                                         "inflation", "human", "finance", "school", "demand", "cash", "bookshelves", "capitaslism", "charge", "power"])

wv_googlenews= api.load("word2vec-google-news-300")

m1.evaluate_word_pairs("evalsim.txt")

m2.evaluate_word_pairs("evalsim.txt")

wv_googlenews.evaluate_word_pairs("evalsim.txt")

#part 4

print("5 most similar words to Dollar using skipGram based Word embedding model: " , m1.most_similar("dollar", topn=5))
print("5 most similar words to Dollar using CBOW based Word embedding model: " , m2.most_similar("dollar", topn=5))
print("5 most similar words to Dollar using pretrained google-news Word embedding model: " , wv_googlenews.most_similar("dollar", topn=5))

print("5 most similar words to market using skipGram based Word embedding model: " , m1.most_similar("market", topn=5))
print("5 most similar words to market using CBOW based Word embedding model: " , m2.most_similar("market", topn=5))
print("5 most similar words to market using pretrained google-news Word embedding model: " , wv_googlenews.most_similar("market", topn=5))

print("5 most similar words to bank using skipGram based Word embedding model: " , m1.most_similar("bank", topn=5))
print("5 most similar words to bank using CBOW based Word embedding model: " , m2.most_similar("bank", topn=5))
print("5 most similar words to bank using pretrained google-news Word embedding model: " , wv_googlenews.most_similar("bank", topn=5))

print("5 most similar words to trade using skipGram based Word embedding model: " , m1.most_similar("trade", topn=5))
print("5 most similar words to trade using CBOW based Word embedding model: " , m2.most_similar("trade", topn=5))
print("5 most similar words to trade using pretrained google-news Word embedding model: " , wv_googlenews.most_similar("trade", topn=5))

print("5 most similar words to release using skipGram based Word embedding model: " , m1.most_similar("release", topn=5))
print("5 most similar words to release using CBOW based Word embedding model: " , m2.most_similar("release", topn=5))
print("5 most similar words to release using pretrained google-news Word embedding model: " , wv_googlenews.most_similar("release", topn=5))